\begin{abstract}
This thesis presents a comprehensive study on Retrieval-Augmented Generation (RAG) methods, focusing on their application to enhance the robustness and reliability of Large Language Models (LLMs). Addressing inherent LLM limitations such as knowledge cutoff and hallucination, RAG integrates external, verifiable knowledge sources to ground generative responses.

We systematically investigate key components of the RAG pipeline, including embedding models, retrieval and reranking techniques, prompt engineering, and LLM selection. Through a series of structured experiments, we demonstrate that optimal RAG system performance is achieved not by isolated component improvements, but through their synergistic combination. Our findings highlight the foundational importance of retrieval quality, showcasing significant gains from advanced embedding models like \texttt{Qwen3-Embedding-0.6B} and the strategic integration of rerankers such as \texttt{GTE ML Reranker Base} with dynamic thresholding. Furthermore, we illustrate the critical impact of generative model choice and meticulous prompt engineering, with newer models like \texttt{O4 Mini} significantly outperforming established baselines when appropriately configured.

This work underscores that building a state-of-the-art RAG system is a multi-faceted engineering challenge requiring careful consideration of performance, cost, and complexity trade-offs. We conclude by outlining promising avenues for future research, including adaptive RAG architectures, advanced chunking and indexing, graph-based RAG, fine-tuning and self-correction mechanisms, and energy efficiency considerations, aiming to further advance the capabilities of trustworthy AI systems.
\end{abstract}