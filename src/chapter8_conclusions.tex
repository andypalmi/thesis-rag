\chapter{Conclusions and Future Work}
\label{chap:conclusions}

This thesis embarked on a comprehensive investigation into RAG methodologies, systematically dissecting the components and strategic approaches essential for constructing more robust and dependable systems with LLMs. Our findings unequivocally demonstrate RAG's efficacy as a powerful paradigm for mitigating the inherent limitations of LLMs, specifically addressing knowledge cutoff and hallucination by grounding responses in external, verifiable information sources.

Our study commenced with an exploration of the fundamental RAG pipeline, elucidating the intricate interplay between its retrieval and generation stages. We then meticulously examined critical optimization techniques applicable at each phase. The experimental outcomes conclusively highlight that optimal RAG system performance is not attributable to any single component in isolation, but rather emerges from the careful tuning and synergistic integration of multiple factors. 

The principal conclusions drawn from our research are as follows:
\begin{itemize}
    \item \textbf{Systematic Optimization is Imperative:} A significant performance disparity was observed between our initial baseline and the finally optimized system, underscoring the critical need for a component-wise approach to RAG pipeline construction. Relying on naive or default configurations, as identified by Gao et al. (2024), is insufficient for achieving optimal results. Our experiments showcased a remarkable improvement in F1 score from 0.021 to 0.654 by transitioning from a baseline \texttt{ada-002} embedding model to \texttt{Qwen3-Embedding-0.6B} and integrating a \texttt{GTE ML Reranker Base} with \texttt{Max Gap} thresholding.
    \item \textbf{Retrieval Quality is Foundational:} Consistent experimental results affirmed that enhancements in the retrieval stage, achieved through superior embedding models and powerful reranking mechanisms, exhibit a profound positive influence on the quality of the final generated answer. The \texttt{Qwen3-Embedding-0.6B} model, our top-performing embedding model, substantially surpassed the \texttt{ada-002} baseline across all evaluated metrics.
    \item \textbf{Reranking Yields Substantial Benefits:} The incorporation of a reranking step, particularly utilizing a sophisticated cross-encoder model such as \texttt{GTE ML Reranker Base}, proved to be one of the most effective strategies for significantly enhancing retrieval precision. This finding validates the crucial role of post-retrieval processing within the Advanced RAG paradigm.
    \item \textbf{Generator Selection and Prompting are Pivotal:} Our extensive evaluation of generative models revealed that the choice of LLM and the adopted prompt style profoundly impact the final output. Newer models, exemplified by \texttt{O4 Mini}, \texttt{GPT-5} and \texttt{Gemini 2.5 Pro}, when judiciously paired with appropriate prompt and temperature settings, demonstrated the capacity to considerably outperform established baselines like GPT-4o in terms of faithfulness, relevancy, and adherence to instructions, achieving score increases exceeding 30 points within our evaluation framework.
\end{itemize}

In essence, this work underscores that developing a state-of-the-art RAG system is a multifaceted engineering endeavor, necessitating careful consideration of the trade-offs among performance, cost, and complexity at every stage of the pipeline.

\section{Future Work}

The domain of Retrieval-Augmented Generation is characterized by rapid evolution, and this study illuminates several promising avenues for future research, many of which align with the directions proposed by Gao et al. (2024):
\begin{itemize}
    \item \textbf{Adaptive RAG Architectures:} Future research could focus on developing systems that dynamically adjust their strategies based on the nature of the query, aligning with the Modular RAG paradigm. This would allow for optimized efficiency and quality, where simple queries trigger streamlined retrieval, while complex queries activate more sophisticated, multi-component pipelines.
    \item \textbf{Advanced Chunking and Indexing:} Further improvements in retrieval relevance could be achieved by exploring more sophisticated, model-based chunking strategies and the implementation of multi-vector indexing, which represents chunks with multiple vectors to capture diverse semantic aspects.
    \item \textbf{Graph-Based RAG Integration:} Investigating the integration of knowledge graphs as the foundational retrieval mechanism could provide more structured and reliable information, particularly within well-defined domains. Future work could explore hybrid systems that combine the strengths of both vector-based and graph-based retrieval approaches.
    \item \textbf{Fine-tuning and Self-Correction Mechanisms:} Developing tighter feedback loops where the generator's output informs the fine-tuning of the retriever and embedding models could lead to self-improving RAG systems. This could involve leveraging reinforcement learning techniques to incentivize the retriever to identify chunks that consistently lead to high-quality answers.
    \item \textbf{Energy Efficiency and Sustainability Considerations:} As RAG systems become increasingly prevalent, it is crucial to investigate the energy consumption associated with various pipeline configurations. Future research should aim to identify and develop methods for constructing efficient yet powerful RAG systems, addressing a key challenge for sustainable AI development.
\end{itemize}

By diligently pursuing these research directions, the community can continue to advance the capabilities of Retrieval-Augmented Generation, thereby paving the way for the development of even more powerful, reliable, and trustworthy AI systems.