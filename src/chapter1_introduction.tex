\chapter{Introduction}
\label{chap:introduction}

The advent of Large Language Models (LLMs) represents a significant advancement in the field of artificial intelligence, exhibiting remarkable capabilities in understanding and generating human-like text. These models, trained on extensive and diverse datasets, serve as foundational components for numerous applications, ranging from sophisticated chatbots to advanced content creation tools. However, their capabilities are subject to certain inherent limitations. Two significant challenges inherent to LLMs are \textit{knowledge cutoff} and \textit{hallucination}. Knowledge cutoff refers to the fact that an LLM's knowledge is static and frozen at the point its training data was collected, rendering it unable to provide information on events or developments that occurred post-training. Hallucination refers to the propensity of these models to produce plausible but factually inaccurate or incoherent information, a consequence of their probabilistic nature and their training objective to predict the next word rather than to assert factual statements.

This thesis investigates Retrieval-Augmented Generation (RAG), a paradigm developed to directly mitigate these limitations \autocite{lewis2020retrieval}. RAG enhances the capabilities of LLMs by grounding their responses in external, up-to-date knowledge sources. Instead of relying solely on its internal, parametric knowledge, a RAG system first retrieves relevant information from a specified corpus—such as a collection of scientific papers, a corporate knowledge base, or the entire web—and then uses this information to inform its generation process. This approach aims to mitigate hallucinations by providing factual context and addresses the knowledge cutoff issue by enabling access to real-time information. The core principle involves integrating the generative capabilities of LLMs with the factual accuracy of information retrieval systems, thereby yielding more robust, reliable, and contextually pertinent responses \autocite{gao2024retrievalaugmented}.

The RAG paradigm has undergone substantial evolution since its inception. As categorized by Gao et al. (2024) \autocite{gao2024retrievalaugmented}, the landscape can be understood through three main stages:
\begin{itemize}
    \item \textbf{Naive RAG:} The initial and most straightforward implementation, involving a simple retrieve-then-read process.
    \item \textbf{Advanced RAG:} Focuses on optimizing the retrieval stage through techniques like pre-retrieval processing and post-retrieval reranking.
    \item \textbf{Modular RAG:} A more flexible and powerful approach that treats RAG as a modular framework, allowing for greater adaptability and integration of different components and patterns.
\end{itemize}

This thesis will systematically examine the foundational Naive RAG architecture and advance towards more sophisticated techniques, primarily focusing on the Advanced RAG paradigm. While Modular RAG is introduced as a concept, its detailed examination falls outside the scope of this thesis, serving instead as a direction for future work. We will conduct a comprehensive analysis of the critical elements comprising the RAG pipeline, including:
\begin{itemize}
    \item \textbf{Embedding Models:} The models used to convert text into numerical representations for semantic search.
    \item \textbf{Retrieval and Reranking:} The techniques for identifying the most relevant information and refining the selection, including reranking and dynamic thresholding.
    \item \textbf{Prompt Engineering:} The methodology of designing effective prompts to guide the LLM in synthesizing the retrieved context.
    \item \textbf{LLM Selection:} The impact of choosing different generator models on the final output.
\end{itemize}

Through a series of structured experiments, this thesis will systematically evaluate the influence of each of these components on system performance, with the objective of establishing a clear framework for constructing and optimizing robust RAG systems.
