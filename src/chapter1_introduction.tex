\chapter{Introduction}
\label{chap:introduction}

The advent of Large Language Models (LLMs) has marked a pivotal moment in the field of artificial intelligence, demonstrating unprecedented capabilities in understanding and generating human-like text. These models, trained on vast and diverse datasets, have become the backbone of numerous applications, from sophisticated chatbots to content creation tools. However, their power is not without limitations. Two significant challenges inherent to LLMs are \textit{knowledge cutoff} and \textit{hallucination}. Knowledge cutoff refers to the fact that an LLM's knowledge is static and frozen at the point its training data was collected, rendering it unable to provide information on events or developments that occurred post-training. Hallucination is the tendency of these models to generate plausible but factually incorrect or nonsensical information, a byproduct of their probabilistic nature and their training objective to predict the next word rather than to state facts.

This thesis delves into the domain of Retrieval-Augmented Generation (RAG), a paradigm designed to directly address these shortcomings. RAG enhances the capabilities of LLMs by grounding their responses in external, up-to-date knowledge sources. Instead of relying solely on its internal, parametric knowledge, a RAG system first retrieves relevant information from a specified corpus—such as a collection of scientific papers, a corporate knowledge base, or the entire web—and then uses this information to inform its generation process. This approach aims to mitigate hallucinations by providing factual context and overcomes the knowledge cutoff issue by allowing access to real-time information. The core principle is to combine the generative power of LLMs with the factual accuracy of information retrieval systems, leading to more robust, trustworthy, and contextually appropriate responses \autocite{rag_survey_2024_ralm}.

This study provides a comprehensive overview of the RAG architecture, exploring its foundational components, advanced optimization techniques, and rigorous evaluation methodologies. We will conduct a deep dive into the critical elements of the RAG pipeline, including:
\begin{itemize}
    \item \textbf{Chunking Strategies:} The methods for segmenting large documents into smaller, digestible pieces for the retriever.
    \item \textbf{Embedding Models:} The models used to convert text into numerical representations for semantic search.
    \item \textbf{Retrieval and Reranking:} The techniques for identifying the most relevant information and refining the selection, including hybrid search methods and dynamic thresholding.
    \item \textbf{Prompt Engineering:} The art of crafting effective prompts to guide the LLM in synthesizing the retrieved context.
    \item \textbf{LLM Selection:} The impact of choosing different generator models on the final output.
\end{itemize}

Through a series of structured experiments, this thesis will systematically evaluate how each of these components influences the overall system performance, with the ultimate goal of providing a clear framework for building and optimizing robust RAG systems.