\chapter{Introduction}
\label{chap:introduction}

This thesis delves into the domain of Retrieval Augmented Generation (RAG), a paradigm aimed at enhancing the capabilities of Large Language Models (LLMs) by grounding their responses in external knowledge sources. While LLMs have demonstrated remarkable proficiency in various natural language tasks, they are often susceptible to issues like hallucination and reliance on outdated internal knowledge \autocite{survey_rag_2024_chen}. [1] RAG seeks to mitigate these limitations by integrating a retrieval step that fetches relevant information from a corpus, which is then used by the LLM to generate more accurate, factual, and contextually appropriate responses \autocite{rag_survey_2024_ralm}. [2]

This study provides a comprehensive overview of RAG, exploring its foundational components, advanced optimization techniques, and evaluation methodologies. We investigate various aspects, from chunking strategies and embedding models to sophisticated reranking mechanisms and dynamic thresholding for similarity scores. Furthermore, the role of prompt engineering and the selection of LLMs are examined for their impact on overall RAG system performance.