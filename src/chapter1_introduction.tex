\chapter{Introduction}
\label{chap:introduction}

The advent of Large Language Models (LLMs) has marked a pivotal moment in the field of artificial intelligence, demonstrating unprecedented capabilities in understanding and generating human-like text. These models, trained on vast and diverse datasets, have become the backbone of numerous applications, from sophisticated chatbots to content creation tools. However, their power is not without limitations. Two significant challenges inherent to LLMs are \textit{knowledge cutoff} and \textit{hallucination}. Knowledge cutoff refers to the fact that an LLM's knowledge is static and frozen at the point its training data was collected, rendering it unable to provide information on events or developments that occurred post-training. Hallucination is the tendency of these models to generate plausible but factually incorrect or nonsensical information, a byproduct of their probabilistic nature and their training objective to predict the next word rather than to state facts.

This thesis investigates Retrieval-Augmented Generation (RAG), a paradigm designed to directly address these shortcomings \autocite{lewis2020retrieval}. RAG enhances the capabilities of LLMs by grounding their responses in external, up-to-date knowledge sources. Instead of relying solely on its internal, parametric knowledge, a RAG system first retrieves relevant information from a specified corpus—such as a collection of scientific papers, a corporate knowledge base, or the entire web—and then uses this information to inform its generation process. This approach aims to mitigate hallucinations by providing factual context and overcomes the knowledge cutoff issue by allowing access to real-time information. The core principle is to combine the generative power of LLMs with the factual accuracy of information retrieval systems, leading to more robust, trustworthy, and contextually appropriate responses \autocite{gao2024retrievalaugmented}.

The RAG paradigm has evolved significantly since its inception. As categorized by Gao et al. (2024) \autocite{gao2024retrievalaugmented}, the landscape can be understood through three main stages:
\begin{itemize}
    \item \textbf{Naive RAG:} The initial and most straightforward implementation, involving a simple retrieve-then-read process.
    \item \textbf{Advanced RAG:} Focuses on optimizing the retrieval stage through techniques like pre-retrieval processing and post-retrieval reranking.
    \item \textbf{Modular RAG:} A more flexible and powerful approach that treats RAG as a modular framework, allowing for greater adaptability and integration of different components and patterns.
\end{itemize}

This thesis will explore these concepts, starting with the foundational Naive RAG architecture and progressing to more advanced techniques. We will conduct a deep dive into the critical elements of the RAG pipeline, including:
\begin{itemize}
    \item \textbf{Embedding Models:} The models used to convert text into numerical representations for semantic search.
    \item \textbf{Retrieval and Reranking:} The techniques for identifying the most relevant information and refining the selection, including reranking and dynamic thresholding.
    \item \textbf{Prompt Engineering:} The art of crafting effective prompts to guide the LLM in synthesizing the retrieved context.
    \item \textbf{LLM Selection:} The impact of choosing different generator models on the final output.
\end{itemize}

Through a series of structured experiments, this thesis will systematically evaluate how each of these components influences system performance, with the ultimate goal of providing a clear framework for building and optimizing robust RAG systems.