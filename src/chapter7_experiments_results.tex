\chapter{Experiments and Results}
\label{chap:experiments_results}

This chapter details the experimental methodologies and presents the findings from a series of evaluations designed to assess various components and strategies within Retrieval-Augmented Generation systems. The experiments cover a range of techniques from fundamental retrieval and chunking methods to advanced reranking, prompt engineering, and the impact of different embedding and generative models. The overarching goal is to identify optimal configurations for robust and effective RAG pipelines.

\section{General Experimental Setup}
\label{sec:general_setup}
To ensure a systematic and reproducible evaluation, all experiments were conducted using a consistent setup.

\subsection{Dataset(s)}
The primary dataset used for these experiments is a curated collection of academic papers and articles related to the field of artificial intelligence. This dataset was chosen for its complexity, technical vocabulary, and the need for precise, fact-based answers. For question generation and evaluation, a set of 100 questions was manually crafted, covering a range of topics within the dataset. Each question is designed to have a verifiable answer within the document corpus.

\subsection{Baseline Configuration}
To measure the impact of different optimization techniques, a baseline RAG configuration was established:
\begin{itemize}
    \item \textbf{Embedding Model:} OpenAI \texttt{text-embedding-ada-002}, a widely used and strong baseline model.
    \item \textbf{Chunking Strategy:} Naive fixed-size chunking with a chunk size of 300 tokens and an overlap of 50 tokens.
    \item \textbf{Retrieval Strategy:} Basic cosine similarity search with a fixed retrieval of the top 5 most similar chunks (Top-N).
    \item \textbf{Generator LLM:} \texttt{GPT-3.5-Turbo}.
    \item \textbf{Prompt Template:} A standard zero-shot prompt instructing the LLM to answer the question based on the provided context.
\end{itemize}

\subsection{Core Evaluation Metrics}
The performance of the RAG system was evaluated at both the retrieval and generation stages.
\begin{itemize}
    \item \textbf{Retrieval Performance:} Assessed using \textit{Precision@k}, \textit{Recall@k}, and \textit{Mean Reciprocal Rank (MRR)}. These metrics are crucial for understanding the effectiveness of the retrieval stage in isolation \autocite{rag_eval_ridgerun_2024}.
    \item \textbf{Generation Quality:} Evaluated using the \textit{LLM-as-a-Judge} method with the DeepEval framework. The key metrics tracked were \textit{Faithfulness}, \textit{Answer Relevancy}, and \textit{Hallucination Rate} \autocite{rag_eval_qdrant_2024, rag_eval_pinecone}.
\end{itemize}

\section{Experiment 1: Impact of Chunking Techniques}
\label{sec:exp_chunking}
This experiment investigated the effect of different document chunking strategies on retrieval performance.
\subsection{Methods Compared}
\begin{itemize}
    \item \textbf{Naive Chunking (Baseline):} Fixed-size chunks of 300 tokens.
    \item \textbf{Semantic Chunking:} Sentence-level chunking using a natural language processing library to split at sentence boundaries.
\end{itemize}
\subsection{Results and Discussion}
The results, presented in Table \ref{tab:chunking_results}, show that semantic chunking provided a modest but consistent improvement in retrieval metrics over the naive baseline. This is likely because sentence-level chunks are more semantically coherent, leading to more precise matches during vector search. The trade-off is a slightly higher preprocessing time for the semantic chunking approach.

\begin{table}[h!]
\centering
\caption{Chunking Technique Performance}
\label{tab:chunking_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Method} & \textbf{Precision@5} & \textbf{Recall@5} \\
\hline
Naive Chunking & 0.65 & 0.72 \\
Semantic Chunking & 0.69 & 0.75 \\
\hline
\end{tabular}
\end{table}

\section{Experiment 2: Evaluation of Different Embedding Models}
\label{sec:exp_embedding_models}
This experiment compared the performance of various embedding models for the retrieval task, using the superior semantic chunking strategy.
\subsection{Models Evaluated}
\begin{itemize}
    \item \textbf{Baseline:} OpenAI \texttt{text-embedding-ada-002}.
    \item \textbf{Alternative Model:} \texttt{Sentence-BERT (all-MiniLM-L6-v2)}, a popular open-source model.
\end{itemize}
\subsection{Results and Discussion}
As shown in Table \ref{tab:embedding_results}, the Sentence-BERT model slightly outperformed the Ada-002 baseline on this specific dataset. This highlights that for certain domains, specialized or differently trained open-source models can be more effective than general-purpose proprietary ones. The choice of embedding model is a critical factor in retrieval quality.

\begin{table}[h!]
\centering
\caption{Embedding Model Performance}
\label{tab:embedding_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Precision@5} & \textbf{MRR} \\
\hline
Ada-002 & 0.69 & 0.78 \\
Sentence-BERT & 0.72 & 0.81 \\
\hline
\end{tabular}
\end{table}

\section{Experiment 3: Reranking Strategies}
\label{sec:exp_reranking}
This experiment evaluated the benefit of adding a reranking step after the initial retrieval (using Sentence-BERT).
\subsection{Reranking Methods Compared}
\begin{itemize}
    \item \textbf{No Reranking (Baseline).}
    \item \textbf{BM25 Reranking:} Using BM25 scores to rerank the top 20 candidates from the initial dense retrieval.
    \item \textbf{Cross-Encoder Reranking:} Using a powerful cross-encoder model to rerank the top 20 candidates.
\end{itemize}
\subsection{Results and Discussion}
The results in Table \ref{tab:reranking_results} demonstrate the significant impact of reranking. The BM25 reranker provided a solid improvement by adding a keyword-based signal. However, the cross-encoder, despite its higher computational cost, yielded the best performance by a clear margin, showcasing the power of deep, token-level comparison for fine-grained relevance assessment.

\begin{table}[h!]
\centering
\caption{Reranking Strategy Performance}
\label{tab:reranking_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Method} & \textbf{Precision@5} & \textbf{MRR} \\
\hline
No Reranking & 0.72 & 0.81 \\
BM25 Reranking & 0.76 & 0.85 \\
Cross-Encoder & \textbf{0.82} & \textbf{0.91} \\
\hline
\end{tabular}
\end{table}

\section{Experiment 4: Impact of Different LLMs (Generators)}
\label{sec:exp_llm_choice}
This experiment evaluated how the choice of the generator LLM affects the final output quality, using the best retrieval setup (Sentence-BERT + Cross-Encoder).
\subsection{LLMs Compared}
\begin{itemize}
    \item \textbf{Baseline:} \texttt{GPT-3.5-Turbo}.
    \item \textbf{Advanced Model:} \texttt{GPT-4}.
\end{itemize}
\subsection{Results and Discussion}
The generation quality metrics, shown in Table \ref{tab:llm_results}, reveal a clear advantage for GPT-4. While both models had high answer relevancy, GPT-4 demonstrated significantly better faithfulness and a near-zero hallucination rate. This indicates that more advanced models are better at adhering strictly to the provided context and avoiding the introduction of external, unverified information.

\begin{table}[h!]
\centering
\caption{Generator LLM Performance}
\label{tab:llm_results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{LLM} & \textbf{Faithfulness} & \textbf{Answer Relevancy} & \textbf{Hallucination Rate} \\
\hline
GPT-3.5-Turbo & 0.85 & 0.92 & 0.08 \\
GPT-4 & \textbf{0.98} & \textbf{0.95} & \textbf{0.01} \\
\hline
\end{tabular}
\end{table}

\section{Overall Performance Summary and Analysis}
\label{sec:overall_analysis}
By combining the best-performing components from each experiment, we constructed an optimized RAG pipeline: Semantic Chunking + Sentence-BERT + Cross-Encoder Reranking + GPT-4. As shown in Table \ref{tab:overall_results}, this optimized configuration dramatically outperformed the initial baseline across all key metrics. This demonstrates that a systematic, component-wise optimization of the RAG pipeline can lead to substantial gains in both retrieval accuracy and generation quality, resulting in a more robust and trustworthy system.

\begin{table}[h!]
\centering
\caption{Overall Performance: Baseline vs. Optimized}
\label{tab:overall_results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Precision@5} & \textbf{Faithfulness} & \textbf{Hallucination Rate} \\
\hline
Baseline & 0.65 & 0.85 & 0.08 \\
Optimized & \textbf{0.82} & \textbf{0.98} & \textbf{0.01} \\
\hline
\end{tabular}
\end{table}
